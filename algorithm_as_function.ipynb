{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickcollins64/opt/anaconda3/envs/MV00/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from utils import get_data, get_probabilities, estimate_q_Z_given_A, get_probabilities_one_hot\n",
    "from sklearn.decomposition import NMF  # Placeholder for volmin factorization\n",
    "from volmin_nmf import *\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from scipy.optimize import nnls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b7c9d1e70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "#                                                     Flags\n",
    "# ======================================================================================================================\n",
    "'''\n",
    "debug: Flag to print debug information such as the shape of the data, examples of data, confusion matrices etc.\n",
    "summary: Flag to print summary information such as accuracy of models and the final q(Y|a)\n",
    "'''\n",
    "debug = False\n",
    "summary = False\n",
    "\n",
    "# ======================================================================================================================\n",
    "#                                           Data Generation Parameters\n",
    "# ======================================================================================================================\n",
    "'''\n",
    "These parameters are used to generate the data.\n",
    "p_source: Probability of a sample being from the source domain\n",
    "p_target: Probability of a sample being from the target domain\n",
    "total: Total number of samples\n",
    "factorisation_atol: Absolute tolerance for the factorisation\n",
    "specific_a_index: Index of the specific A value to be used\n",
    "num_classes_Y: Number of classes for Y\n",
    "num_classes_W: Number of classes for W\n",
    "num_features_Z: Number of features for Z\n",
    "num_features_A: Number of features for A\n",
    "'''\n",
    "p_target = 0.2\n",
    "p_source = 1 - p_target\n",
    "total = 10000\n",
    "factorisation_atol = 1e-1\n",
    "specific_a_index = 0  # First value of A\n",
    "num_classes_Y = 2 \n",
    "num_classes_W = 2\n",
    "num_features_Z = 2\n",
    "num_features_A = 3\n",
    "num_epsilon = 2 # min(W_source.shape[1], Z_source.shape[1]) # Remember we need this to be less than the min of |W| and |Z|. Consider changing this as a hyperparameter\n",
    "\n",
    "# ======================================================================================================================\n",
    "#                                                NMF Parameters\n",
    "# ======================================================================================================================\n",
    "'''\n",
    "Parameters for the NMF factorisation\n",
    "nmf_method: Method for the NMF factorisation (currently uses https://github.com/bm424/mvcnmf/tree/master and activated by \"volmin_2\")\n",
    "w_vol: Volume regularisation parameter\n",
    "delta: Delta parameter for the NMF factorisation\n",
    "n_iter: Number of iterations for the NMF factorisation\n",
    "err_cut: Error cut-off for the NMF factorisation\n",
    "'''\n",
    "nmf_method = \"volmin_2\" \n",
    "w_vol = 0.01#0.1\n",
    "delta = 1e-8\n",
    "n_iter = 200000\n",
    "err_cut = 1e-10\n",
    "\n",
    "# ======================================================================================================================\n",
    "#                                           Random Seed Initialisation\n",
    "# ======================================================================================================================\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "#                                          Matrices for Data Generation\n",
    "# ======================================================================================================================\n",
    "'''\n",
    "These matrices are used to generate the data.\n",
    "theta_w_epsilon: Matrix for W given Epsilon\n",
    "theta_z_epsilon: Matrix for Z given Epsilon\n",
    "theta_a_z_epsilon: Matrix for A given Z and Epsilon\n",
    "theta_y_a_w_epsilon: Matrix for Y given A, W and Epsilon\n",
    "\n",
    "The dimensions of the variables are as follows:\n",
    "W: 2D\n",
    "Z: 2D\n",
    "A: 3D\n",
    "Y: 2D\n",
    "Epsilon: 2D\n",
    "'''\n",
    "\n",
    "theta_w_epsilon = torch.tensor([\n",
    "    [-2, 2],\n",
    "    [2, -2]\n",
    "])\n",
    "\n",
    "theta_z_epsilon = torch.tensor([\n",
    "    [-2, 2],\n",
    "    [2, -2]\n",
    "])\n",
    "\n",
    "theta_a_z_epsilon = torch.tensor([\n",
    "    [2.0, -2.0],  \n",
    "    [-2.0, 2.0],\n",
    "    [2.0, -2.0]\n",
    "]) \n",
    "\n",
    "# 1x5 matrix concatenating 1 hot of w and a. Negate when epsilon is 1. a is 3D and w is 2D\n",
    "theta_y_a_w_epsilon = torch.tensor([\n",
    "    [10, 3, -5, -3, 3]\n",
    "]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "#                                          Data Generation Functions\n",
    "# ======================================================================================================================\n",
    "\n",
    "def get_tuple_new(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, p=p_target):\n",
    "    '''\n",
    "    Generates a tuple of data points for the given parameters.\n",
    "\n",
    "    Parameters:\n",
    "    theta_w_epsilon: Matrix for W given Epsilon\n",
    "    theta_z_epsilon: Matrix for Z given Epsilon\n",
    "    theta_a_z_epsilon: Matrix for A given Z and Epsilon\n",
    "    theta_y_a_w_epsilon: Matrix for Y given A, W and Epsilon\n",
    "    p: Probability of a sample being from the target domain\n",
    "\n",
    "    Returns:\n",
    "    epsilon: Epsilon value via Bernoulli distribution\n",
    "    z: Z value via Bernoulli distribution and sigmoid function\n",
    "    w: W value via Bernoulli distribution and sigmoid function\n",
    "    a: A value via theta_a_z_epsilon @ z or -theta_a_z_epsilon @ z depending on epsilon. One-hot encoded\n",
    "    y: Y value via Bernoulli distribution and sigmoid function of theta_y_a_w_epsilon @ [w, a] or -theta_y_a_w_epsilon @ [w, a] depending on epsilon\n",
    "    '''\n",
    "\n",
    "    epsilon = torch.bernoulli(torch.tensor([p])).long() # (1,)\n",
    "    epsilon_one_hot = torch.nn.functional.one_hot(epsilon.clone().detach(), num_classes=2).squeeze() # (1,2)\n",
    "\n",
    "    z = torch.bernoulli(torch.sigmoid(epsilon_one_hot @ theta_z_epsilon)).float().squeeze()\n",
    "    w = torch.bernoulli(torch.sigmoid(epsilon_one_hot @ theta_w_epsilon)).float().squeeze() # w given z and a should be perfect\n",
    "\n",
    "    # w given z and a should be perfect\n",
    "\n",
    "    a_logits = epsilon * theta_a_z_epsilon.float() @ z.float() + (epsilon-1) * theta_a_z_epsilon.float() @ z.float() #epsilon is 0 or 1, theta_a_z_epsilon is 3x2, z is [2]. So result is 3x1\n",
    "    a_prob = torch.softmax(a_logits, dim=0)  # Apply softmax to convert logits to probabilities\n",
    "    if debug:\n",
    "        print(a_prob)   \n",
    "    a_category = torch.multinomial(a_prob, 1).squeeze()  # Sample from the categorical distribution\n",
    "    a = torch.nn.functional.one_hot(a_category, num_classes=3).float()  # Convert to one-hot encoding\n",
    "    if debug:\n",
    "        print(a)\n",
    "    # a = torch.bernoulli(torch.sigmoid(a_logits)).float().squeeze() # a is size 3\n",
    "\n",
    "    # concatenate w and a\n",
    "    # then similar but use different matrix and w,a instead of z\n",
    "    wa = torch.cat((w, a), dim=0) # [5]\n",
    "\n",
    "    y_logits = epsilon * theta_y_a_w_epsilon @ wa.long() + (epsilon-1) * theta_y_a_w_epsilon @ wa.long() # y is a function of a, w and epsilon from the graph\n",
    "    #print(torch.sigmoid(y_logits))\n",
    "    y = torch.bernoulli(torch.sigmoid(y_logits).squeeze()).float()\n",
    "\n",
    "    return epsilon,z,w,a,y\n",
    "\n",
    "\n",
    "def get_dataset_new(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, p, total):\n",
    "    '''\n",
    "    Generates a dataset of data points for the given parameters, using the get_tuple_new function.\n",
    "\n",
    "    Parameters:\n",
    "    theta_w_epsilon: Matrix for W given Epsilon\n",
    "    theta_z_epsilon: Matrix for Z given Epsilon\n",
    "    theta_a_z_epsilon: Matrix for A given Z and Epsilon\n",
    "    theta_y_a_w_epsilon: Matrix for Y given A, W and Epsilon\n",
    "    p: Probability of a sample being from the target domain\n",
    "    total: Total number of samples\n",
    "\n",
    "    Returns:\n",
    "    U: List of Epsilon values\n",
    "    Z: List of Z values\n",
    "    W: List of W values\n",
    "    X: List of A values\n",
    "    Y: List of Y values\n",
    "    '''\n",
    "\n",
    "    U,Z,W,X,Y = [],[],[],[],[]\n",
    "    for _ in range(total):\n",
    "        u,z,w,x,y = get_tuple_new(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, p)\n",
    "        U.append(u)\n",
    "        Z.append(z)\n",
    "        W.append(w)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return U,Z,W,X,Y\n",
    "\n",
    "\n",
    "def get_data_new(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, p_target, total):\n",
    "    '''\n",
    "    Returns the source and target datasets for the given parameters.\n",
    "\n",
    "    Parameters:\n",
    "    theta_w_epsilon: Matrix for W given Epsilon\n",
    "    theta_z_epsilon: Matrix for Z given Epsilon\n",
    "    theta_a_z_epsilon: Matrix for A given Z and Epsilon\n",
    "    theta_y_a_w_epsilon: Matrix for Y given A, W and Epsilon\n",
    "    p_target: Probability of a sample being from the target domain\n",
    "    total: Total number of samples\n",
    "\n",
    "    Returns:\n",
    "    (Z_source, U_source, W_source, X_source, Y_source): Source dataset\n",
    "    (Z_target, U_target, W_target, X_target, Y_target): Target dataset\n",
    "    '''\n",
    "    \n",
    "    # Source distribution data\n",
    "    U_source, Z_source, W_source, X_source, Y_source = get_dataset_new(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, 1-p_target, total)\n",
    "    \n",
    "    # Target distribution data\n",
    "    U_target, Z_target, W_target, X_target, Y_target = get_dataset_new(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, p_target, total)\n",
    "    \n",
    "    return (Z_source, U_source, W_source, X_source, Y_source), \\\n",
    "           (Z_target, U_target, W_target, X_target, Y_target)\n",
    "\n",
    "def tensor_to_binary(tensor):\n",
    "    return 1 if tensor[0] == 1 else 0\n",
    "\n",
    "def get_probabilities(model, Z_source, A_source):\n",
    "    '''\n",
    "    Function to get the probabilities of Y given all possible Z and A and put them in a 3D array.\n",
    "    '''\n",
    "\n",
    "    # Convert lists of tensors to numpy arrays\n",
    "    Z = np.array([z.numpy() for z in Z_source])\n",
    "    A = np.array([a.numpy() for a in A_source])\n",
    "    \n",
    "    num_Z = Z.shape[1]\n",
    "    num_A = A.shape[1]\n",
    "\n",
    "    # Generate all possible one-hot vectors for Z and A\n",
    "    possible_Z = np.eye(num_Z)\n",
    "    possible_A = np.eye(num_A)\n",
    "    \n",
    "    probabilities = []\n",
    "    \n",
    "    for z in possible_Z:\n",
    "        for a in possible_A:\n",
    "            ZA = np.hstack((z.reshape(1, -1), a.reshape(1, -1)))\n",
    "            # Flatten ZA for the MLPClassifier\n",
    "            ZA_flat = ZA.flatten().reshape(1, -1)\n",
    "            prob = model.predict_proba(ZA_flat)[0]\n",
    "            probabilities.append(prob)\n",
    "    \n",
    "    probabilities = np.array(probabilities).reshape((num_Z, num_A, 2))\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, p_source, p_target, total, specific_a_index, num_classes_Y, num_classes_W, num_features_Z, num_features_A, num_epsilon, nmf_method, w_vol, delta, n_iter, err_cut, summary = True):\n",
    "    '''Runs the full algorithm'''\n",
    "\n",
    "    # Generate the source and target datasets\n",
    "    source_data, target_data = get_data_new(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, p_target, total)\n",
    "    Z_source, epsilon_source, W_source, A_source, Y_source = source_data\n",
    "    Z_target, epsilon_target, W_target, A_target, Y_target = target_data\n",
    "\n",
    "    # Split source data into training, validation, and test sets\n",
    "    A_train_val_source, A_test_source, Z_train_val_source, Z_test_source, W_train_val_source, W_test_source, Y_train_val_source, Y_test_source, epsilon_train_val_source, epsilon_test_source = train_test_split(\n",
    "    A_source, Z_source, W_source, Y_source, epsilon_source, test_size=0.2, random_state=42)\n",
    "    A_train_source, A_val_source, Z_train_source, Z_val_source, W_train_source, W_val_source, Y_train_source, Y_val_source, epsilon_train_source, epsilon_val_source = train_test_split(\n",
    "    A_train_val_source, Z_train_val_source, W_train_val_source, Y_train_val_source, epsilon_train_val_source, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Split target data into training, validation, and test sets\n",
    "    A_train_val_target, A_test_target, Z_train_val_target, Z_test_target, W_train_val_target, W_test_target, Y_train_val_target, Y_test_target, epsilon_train_val_target, epsilon_test_target = train_test_split(\n",
    "    A_target, Z_target, W_target, Y_target, epsilon_target, test_size=0.2, random_state=42)\n",
    "    A_train_target, A_val_target, Z_train_target, Z_val_target, W_train_target, W_val_target, Y_train_target, Y_val_target, epsilon_train_target, epsilon_val_target = train_test_split(\n",
    "    A_train_val_target, Z_train_val_target, W_train_val_target, Y_train_val_target, epsilon_train_val_target, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Step 1\n",
    "    ZA_source = np.hstack((Z_train_source, A_train_source)) \n",
    "    model_Y = SklearnLogisticRegression(max_iter=1000)\n",
    "    model_Y.fit(ZA_source, Y_train_source)\n",
    "    Y_train_pred = model_Y.predict(ZA_source)\n",
    "    Y_train_true = Y_train_source\n",
    "    accuracy_Y_train = np.mean(Y_train_pred == Y_train_source)\n",
    "    # test set\n",
    "    ZA_source_test = np.hstack((Z_test_source, A_test_source))\n",
    "    Y_test_pred = model_Y.predict(ZA_source_test)\n",
    "    Y_test_true = Y_test_source\n",
    "    accuracy_Y_test = np.mean(Y_test_pred == Y_test_true)\n",
    "    # W \n",
    "    # Convert the list of tensors to a list of binary labels\n",
    "    binary_labels = [tensor_to_binary(t) for t in W_train_source]\n",
    "\n",
    "    # Convert to numpy array (optional)\n",
    "    binary_labels_array = np.array(binary_labels)\n",
    "\n",
    "    model_W= SklearnLogisticRegression(max_iter=1000)\n",
    "    model_W.fit(ZA_source, binary_labels_array)\n",
    "    W_train_pred = model_W.predict(ZA_source)\n",
    "    W_train_true = W_source\n",
    "    accuracy_W_train = np.mean(W_train_pred == binary_labels_array)\n",
    "    # Test set\n",
    "    W_test_pred = model_W.predict(ZA_source_test)\n",
    "    W_test_true = np.array([tensor_to_binary(t) for t in W_test_source])\n",
    "    accuracy_W_test = np.mean(W_test_pred == W_test_true)\n",
    "    p_Y_given_ZA = get_probabilities(model_Y, Z_source, A_source)\n",
    "    p_Y_given_ZA_matrix = p_Y_given_ZA[:, specific_a_index, :]\n",
    "    p_W_given_ZA = get_probabilities(model_W, Z_source, A_source)\n",
    "    p_W_given_ZA_matrix = p_W_given_ZA[:, specific_a_index, :]\n",
    "    if debug:\n",
    "        print(\"Step 1 done\")\n",
    "\n",
    "    # Step 2\n",
    "    stacked_matrix = np.vstack((p_Y_given_ZA_matrix, p_W_given_ZA_matrix)) # this should be a |Y| x |Z| matrix stacked on top of a |W| x |Z| matrix (for specific a)\n",
    "    W, H = mvc_nmf(stacked_matrix.T, num_epsilon, w_vol, n_iter, err_cut) # Transpose the matrix to match the input format of the function\n",
    "    p_Y_given_epsilon = W[:num_classes_Y, :] # |Y| x |\\Epsilon| matrix for specific a, the first num_classes_Y rows #CHECK NUM_CLASSES_Y IS THE ONE\n",
    "    p_W_given_epsilon = W[num_classes_Y:, :] # |W| x |\\Epsilon| matrix for specific a, the rest of the rows #CHECK NUM_CLASSES_Y IS THE ONE\n",
    "    p_epsilon_given_ZA = H # |\\Epsilon| x |Z| matrix for specific a\n",
    "    reconstructed_stacked_matrix = np.dot(W, H)\n",
    "    if debug:\n",
    "        print(\"Step 2 done\")\n",
    "\n",
    "    # Step 3\n",
    "    ZA_train_target = np.hstack((Z_train_target, A_train_target))\n",
    "    # Convert the list of tensors to a list of binary labels\n",
    "    binary_labels_target = [tensor_to_binary(t) for t in W_train_target]\n",
    "    # Convert to numpy array (optional)\n",
    "    binary_labels_target_array = np.array(binary_labels_target)\n",
    "    model_q_W = SklearnLogisticRegression(max_iter=1000)\n",
    "    model_q_W.fit(ZA_train_target, binary_labels_target_array)\n",
    "    W_train_pred_target = model_q_W.predict(ZA_train_target)\n",
    "    W_train_true_target = W_train_target\n",
    "    accuracy_W_train_target = np.mean(W_train_pred_target == binary_labels_target_array)\n",
    "    q_W_given_ZA = get_probabilities(model_q_W, Z_target, A_target)\n",
    "    if debug:\n",
    "        print(\"Step 3 done\")\n",
    "\n",
    "    # Step 4\n",
    "    q_W_given_ZA_specific_a = q_W_given_ZA[:, specific_a_index, :]\n",
    "    q_epsilon_given_Z_and_A, _, _, _ = np.linalg.lstsq(p_W_given_epsilon, q_W_given_ZA_specific_a, rcond=None) \n",
    "    q_epsilon_given_Z_and_A #note I'm getting a negative value here\n",
    "    if debug:\n",
    "        print(\"Step 4 done\")\n",
    "\n",
    "    # Step 5\n",
    "    # Convert the list of tensors to a list of binary labels\n",
    "    binary_labels_target_Z = [tensor_to_binary(t) for t in Z_train_target]\n",
    "    # Convert to numpy array (optional)\n",
    "    binary_labels_target_array_Z = np.array(binary_labels_target_Z)\n",
    "    model_q_Z = SklearnLogisticRegression(max_iter=1000)#, class_weight = class_weights)\n",
    "    model_q_Z.fit(A_train_target, binary_labels_target_array_Z)\n",
    "    Z_train_pred_target = model_q_Z.predict(A_train_target)\n",
    "    Z_train_true_target = Z_train_target\n",
    "    accuracy_Z_train_target = np.mean(Z_train_pred_target == binary_labels_target_array_Z)\n",
    "    if debug:\n",
    "        print(\"Step 5 done\")\n",
    "\n",
    "    # Step 6\n",
    "    one_hot_specific_a = np.eye(len(A_source[0]))[specific_a_index].reshape(1, -1)\n",
    "    q_Z_given_A = model_q_Z.predict_proba(one_hot_specific_a)\n",
    "    q_Y_given_A = p_Y_given_epsilon@(q_epsilon_given_Z_and_A@q_Z_given_A.T)\n",
    "    q_Y_given_a_normalised = q_Y_given_A / np.sum(q_Y_given_A)\n",
    "    if debug:\n",
    "        print(\"Step 6 done\")\n",
    "\n",
    "    if summary:\n",
    "        print(\"Summary:\")\n",
    "        print(f\"Accuracy of model_Y on training set: {accuracy_Y_train * 100:.2f}%\")\n",
    "        print(f\"Accuracy of model_Y on test set: {accuracy_Y_test * 100:.2f}%\")\n",
    "        print(f\"Accuracy of model_W on training set: {accuracy_W_train * 100:.2f}%\")\n",
    "        print(f\"Accuracy of model_W on test set: {accuracy_W_test * 100:.2f}%\")\n",
    "        print(f\"Accuracy of model_W on training set (target): {accuracy_W_train_target * 100:.2f}%\")\n",
    "        print(f\"Accuracy of model_W on test set (target): {accuracy_W_test * 100:.2f}%\")\n",
    "        print(f\"Accuracy of model on training set (target): {accuracy_Z_train_target * 100:.2f}%\")\n",
    "\n",
    "    return q_Y_given_a_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run algorithm for different values of A and p_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_target_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # Example values, you can define your own range\n",
    "specific_a_index_values = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for p_target=0.1, specific_a_index=0: [[0.34939042]\n",
      " [0.64458964]]\n",
      "Result for p_target=0.1, specific_a_index=1: [[0.50023259]\n",
      " [0.45286618]]\n",
      "Result for p_target=0.1, specific_a_index=2: [[0.29635302]\n",
      " [0.60812819]]\n",
      "Result for p_target=0.2, specific_a_index=0: [[0.31559285]\n",
      " [0.54537215]]\n",
      "Result for p_target=0.2, specific_a_index=1: [[0.52111521]\n",
      " [0.46535484]]\n",
      "Result for p_target=0.2, specific_a_index=2: [[0.32371805]\n",
      " [0.61364591]]\n",
      "Result for p_target=0.3, specific_a_index=0: [[0.36231379]\n",
      " [0.49883146]]\n",
      "Result for p_target=0.3, specific_a_index=1: [[0.51544238]\n",
      " [0.48656528]]\n",
      "Result for p_target=0.3, specific_a_index=2: [[0.37602944]\n",
      " [0.59939354]]\n",
      "Result for p_target=0.4, specific_a_index=0: [[0.41598081]\n",
      " [0.48127673]]\n",
      "Result for p_target=0.4, specific_a_index=1: [[0.50170367]\n",
      " [0.49889235]]\n",
      "Result for p_target=0.4, specific_a_index=2: [[0.44537426]\n",
      " [0.55072019]]\n",
      "Result for p_target=0.5, specific_a_index=0: [[0.50590158]\n",
      " [0.50154415]]\n",
      "Result for p_target=0.5, specific_a_index=1: [[0.48247332]\n",
      " [0.5088466 ]]\n",
      "Result for p_target=0.5, specific_a_index=2: [[0.50464092]\n",
      " [0.49521507]]\n",
      "Result for p_target=0.6, specific_a_index=0: [[0.59231261]\n",
      " [0.53371801]]\n",
      "Result for p_target=0.6, specific_a_index=1: [[0.50572676]\n",
      " [0.49834392]]\n",
      "Result for p_target=0.6, specific_a_index=2: [[0.55163884]\n",
      " [0.44436381]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m p_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p_target\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m specific_a_index \u001b[38;5;129;01min\u001b[39;00m specific_a_index_values:\n\u001b[0;32m----> 4\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_w_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_z_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_a_z_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_y_a_w_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecific_a_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features_Z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnmf_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_vol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_cut\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Process the result as needed\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult for p_target=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, specific_a_index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspecific_a_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mrun_algorithm\u001b[0;34m(theta_w_epsilon, theta_z_epsilon, theta_a_z_epsilon, theta_y_a_w_epsilon, p_source, p_target, total, specific_a_index, num_classes_Y, num_classes_W, num_features_Z, num_features_A, num_epsilon, nmf_method, w_vol, delta, n_iter, err_cut, summary)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Runs the full algorithm'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Generate the source and target datasets\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m source_data, target_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_w_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_z_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_a_z_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_y_a_w_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m Z_source, epsilon_source, W_source, A_source, Y_source \u001b[38;5;241m=\u001b[39m source_data\n\u001b[1;32m      7\u001b[0m Z_target, epsilon_target, W_target, A_target, Y_target \u001b[38;5;241m=\u001b[39m target_data\n",
      "Cell \u001b[0;32mIn[4], line 106\u001b[0m, in \u001b[0;36mget_data_new\u001b[0;34m(theta_w_epsilon, theta_z_epsilon, theta_a_z_epsilon, theta_y_a_w_epsilon, p_target, total)\u001b[0m\n\u001b[1;32m    103\u001b[0m U_source, Z_source, W_source, X_source, Y_source \u001b[38;5;241m=\u001b[39m get_dataset_new(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mp_target, total)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Target distribution data\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m U_target, Z_target, W_target, X_target, Y_target \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_w_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_z_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_a_z_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_y_a_w_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (Z_source, U_source, W_source, X_source, Y_source), \\\n\u001b[1;32m    109\u001b[0m        (Z_target, U_target, W_target, X_target, Y_target)\n",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m, in \u001b[0;36mget_dataset_new\u001b[0;34m(theta_w_epsilon, theta_z_epsilon, theta_a_z_epsilon, theta_y_a_w_epsilon, p, total)\u001b[0m\n\u001b[1;32m     73\u001b[0m U,Z,W,X,Y \u001b[38;5;241m=\u001b[39m [],[],[],[],[]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total):\n\u001b[0;32m---> 75\u001b[0m     u,z,w,x,y \u001b[38;5;241m=\u001b[39m \u001b[43mget_tuple_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_w_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_z_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_a_z_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_y_a_w_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     U\u001b[38;5;241m.\u001b[39mappend(u)\n\u001b[1;32m     77\u001b[0m     Z\u001b[38;5;241m.\u001b[39mappend(z)\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mget_tuple_new\u001b[0;34m(theta_w_epsilon, theta_z_epsilon, theta_a_z_epsilon, theta_y_a_w_epsilon, p)\u001b[0m\n\u001b[1;32m     24\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(torch\u001b[38;5;241m.\u001b[39mtensor([p]))\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;66;03m# (1,)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m epsilon_one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(epsilon\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach(), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;66;03m# (1,2)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[43mepsilon_one_hot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtheta_z_epsilon\u001b[49m))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     28\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(torch\u001b[38;5;241m.\u001b[39msigmoid(epsilon_one_hot \u001b[38;5;241m@\u001b[39m theta_w_epsilon))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;66;03m# w given z and a should be perfect\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# w given z and a should be perfect\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for p_target in p_target_values:\n",
    "    p_source = 1 - p_target\n",
    "    for specific_a_index in specific_a_index_values:\n",
    "        result = run_algorithm(theta_w_epsilon,theta_z_epsilon,theta_a_z_epsilon,theta_y_a_w_epsilon, p_source, p_target, total, specific_a_index, num_classes_Y, num_classes_W, num_features_Z, num_features_A, num_epsilon, nmf_method, w_vol, delta, n_iter, err_cut, summary = False)\n",
    "        # Process the result as needed\n",
    "        print(f\"Result for p_target={p_target}, specific_a_index={specific_a_index}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MV00",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
