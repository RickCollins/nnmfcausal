{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from utils import get_data, get_probabilities, estimate_q_Z_given_A, get_probabilities_one_hot\n",
    "from sklearn.decomposition import NMF  # Placeholder for volmin factorization\n",
    "from volmin_nmf import *\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from data import get_dataset, get_dataset_new\n",
    "from scipy.linalg import sqrtm, det, pinv\n",
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data(theta_a_z, theta_y_a, theta_y_w, p_source, p_target, total):\n",
    "    \"\"\"\n",
    "    Generates datasets for source and target distributions.\n",
    "    \n",
    "    Args:\n",
    "        theta_xz (torch.Tensor): Transformation matrix for X given Z.\n",
    "        theta_yx (torch.Tensor): Transformation matrix for Y given X.\n",
    "        theta_yw (torch.Tensor): Transformation matrix for Y given W.\n",
    "        p_source (float): Probability parameter for the source distribution.\n",
    "        p_target (float): Probability parameter for the target distribution.\n",
    "        total (int): Total number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two tuples containing the source and target datasets respectively.\n",
    "    \"\"\"\n",
    "    # Source distribution data\n",
    "    U_source, Z_source, W_source, X_source, Y_source = get_dataset(theta_a_z,theta_y_a,theta_y_w, p_source, total)\n",
    "    \n",
    "    # Target distribution data\n",
    "    U_target, Z_target, W_target, X_target, Y_target = get_dataset(theta_a_z,theta_y_a,theta_y_w, p_target, total)\n",
    "    \n",
    "    return (Z_source.numpy(), U_source.numpy(), W_source.numpy(), X_source.numpy(), Y_source.numpy()), \\\n",
    "           (Z_target.numpy(), U_target.numpy(), W_target.numpy(), X_target.numpy(), Y_target.numpy())\n",
    "\n",
    "def get_data_new(theta_a_z, theta_y_a, theta_y_w, theta_y_epsilon, theta_a_epsilon, p_source, p_target, total):\n",
    "    \"\"\"\n",
    "    Generates datasets for source and target distributions.\n",
    "    \n",
    "    Args:\n",
    "        theta_xz (torch.Tensor): Transformation matrix for X given Z.\n",
    "        theta_yx (torch.Tensor): Transformation matrix for Y given X.\n",
    "        theta_yw (torch.Tensor): Transformation matrix for Y given W.\n",
    "        p_source (float): Probability parameter for the source distribution.\n",
    "        p_target (float): Probability parameter for the target distribution.\n",
    "        total (int): Total number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two tuples containing the source and target datasets respectively.\n",
    "    \"\"\"\n",
    "    # Source distribution data\n",
    "    U_source, Z_source, W_source, X_source, Y_source = get_dataset_new(theta_a_z, theta_y_a, theta_y_w, theta_y_epsilon, theta_a_epsilon, p_source, total)\n",
    "    \n",
    "    # Target distribution data\n",
    "    U_target, Z_target, W_target, X_target, Y_target = get_dataset_new(theta_a_z, theta_y_a, theta_y_w, theta_y_epsilon, theta_a_epsilon, p_target, total)\n",
    "    \n",
    "    return (Z_source.numpy(), U_source.numpy(), W_source.numpy(), X_source.numpy(), Y_source.numpy()), \\\n",
    "           (Z_target.numpy(), U_target.numpy(), W_target.numpy(), X_target.numpy(), Y_target.numpy())\n",
    "\n",
    "\n",
    "def get_probabilities(model, Z, A):\n",
    "    \"\"\"\n",
    "    Computes the softmax probabilities from the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model (model): Trained model.\n",
    "        Z (numpy.ndarray): Feature matrix Z.\n",
    "        A (numpy.ndarray): Feature matrix A.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Probability matrix reshaped to (|Z|, |A|, |Y|) or (|Z|, |A|, |W|).\n",
    "    \"\"\"\n",
    "    num_Z = Z.shape[1]\n",
    "    num_A = A.shape[1]\n",
    "    num_classes = model.linear.out_features\n",
    "\n",
    "    # Generate all possible one-hot vectors for Z\n",
    "    possible_Z = np.eye(num_Z)\n",
    "    possible_A = np.eye(num_A)\n",
    "    \n",
    "    probabilities = []\n",
    "    \n",
    "    for z in possible_Z:\n",
    "        for a in possible_A:\n",
    "            ZA = np.hstack((z.reshape(1, -1), a.reshape(1, -1)))\n",
    "            ZA_tensor = torch.tensor(ZA, dtype=torch.float32)\n",
    "            print(ZA_tensor)\n",
    "            with torch.no_grad():\n",
    "                logits = model(ZA_tensor)\n",
    "                print(logits)\n",
    "                prob_1 = torch.sigmoid(logits).numpy()[0][0]  # Probability of class 1\n",
    "                prob_0 = 1 - prob_1  # Probability of class 0\n",
    "                print([prob_0, prob_1])\n",
    "                probabilities.append([prob_0, prob_1])\n",
    "    \n",
    "    probabilities = np.array(probabilities).reshape((num_Z, num_A, 2))\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def get_probabilities_one_hot(model, Z, A):\n",
    "    \"\"\"\n",
    "    Computes the softmax probabilities from the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model (model): Trained model.\n",
    "        Z (numpy.ndarray): Feature matrix Z.\n",
    "        A (numpy.ndarray): Feature matrix A.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Probability matrix reshaped to (|Z|, |A|, |Y|) or (|Z|, |A|, |W|).\n",
    "    \"\"\"\n",
    "    num_Z = Z.shape[1]\n",
    "    num_A = A.shape[1]\n",
    "    num_classes = model.linear.out_features\n",
    "\n",
    "    # Generate all possible one-hot vectors for Z\n",
    "    possible_Z = np.eye(num_Z)\n",
    "    possible_A = np.eye(num_A)\n",
    "    \n",
    "    probabilities = []\n",
    "    \n",
    "    for z in possible_Z:\n",
    "        for a in possible_A:\n",
    "            ZA = np.hstack((z.reshape(1, -1), a.reshape(1, -1)))\n",
    "            ZA_tensor = torch.tensor(ZA, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                probs = model(ZA_tensor)\n",
    "                probs = torch.softmax(probs, dim=1).numpy()\n",
    "                probabilities.append(probs[0])\n",
    "    \n",
    "    probabilities = np.array(probabilities).reshape((num_Z, num_A, num_classes))\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def get_probabilities_one_hot_nn(model, Z, A):\n",
    "    \"\"\"\n",
    "    Computes the softmax probabilities from the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model (model): Trained model.\n",
    "        Z (numpy.ndarray): Feature matrix Z.\n",
    "        A (numpy.ndarray): Feature matrix A.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Probability matrix reshaped to (|Z|, |A|, |Y|) or (|Z|, |A|, |W|).\n",
    "    \"\"\"\n",
    "    num_Z = Z.shape[1]\n",
    "    num_A = A.shape[1]\n",
    "    num_classes = model.fc3.out_features\n",
    "\n",
    "    # Generate all possible one-hot vectors for Z\n",
    "    possible_Z = np.eye(num_Z)\n",
    "    possible_A = np.eye(num_A)\n",
    "    \n",
    "    probabilities = []\n",
    "    \n",
    "    for z in possible_Z:\n",
    "        for a in possible_A:\n",
    "            ZA = np.hstack((z.reshape(1, -1), a.reshape(1, -1)))\n",
    "            ZA_tensor = torch.tensor(ZA, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                probs = model(ZA_tensor)\n",
    "                probs = torch.softmax(probs, dim=1).numpy()\n",
    "                probabilities.append(probs[0])\n",
    "    \n",
    "    probabilities = np.array(probabilities).reshape((num_Z, num_A, num_classes))\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def estimate_q_Z_given_A(model, A, num_classes_Z, num_features_A):\n",
    "    \"\"\"\n",
    "    Estimate q(Z|a) using the given model.\n",
    "    \n",
    "    Args:\n",
    "        model (model): Trained model.\n",
    "        A (numpy.ndarray): Feature matrix A.\n",
    "        num_classes_Z (int): Number of classes for Z.\n",
    "        num_features_A (int): Number of features for A.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Estimated q(Z|a) matrix.\n",
    "    \"\"\"\n",
    "    num_A = A.shape[1]\n",
    "    num_classes = model.linear.out_features\n",
    "\n",
    "    # Generate all possible one-hot vectors for A\n",
    "    possible_A = np.eye(num_A)\n",
    "    \n",
    "    probabilities = []\n",
    "    \n",
    "    for a in possible_A:\n",
    "        A_sample = a.reshape(1, -1)\n",
    "        A_tensor = torch.tensor(A_sample, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            probs = model(A_tensor)\n",
    "            probs = torch.softmax(probs, dim=1).numpy()\n",
    "            probabilities.append(probs)\n",
    "    \n",
    "    probabilities = np.array(probabilities).reshape((num_features_A, num_classes_Z))\n",
    "    return probabilities\n",
    "\n",
    "# linear solve for Q(Epsilon | Z, A) using linalg.lstsq\n",
    "def solve_for_q_epsilon_given_ZA(p_W_given_epsilon, q_W_given_ZA):\n",
    "    print(\"p_W_given_epsilon shape before transpose:\", p_W_given_epsilon.shape)  # Debug\n",
    "    print(\"q_W_given_ZA shape before transpose:\", q_W_given_ZA.shape)  # Debug\n",
    "    p_W_given_epsilon = p_W_given_epsilon.T\n",
    "    q_W_given_ZA = q_W_given_ZA.T\n",
    "    print(\"p_W_given_epsilon shape after transpose:\", p_W_given_epsilon.shape)  # Debug\n",
    "    print(\"q_W_given_ZA shape after transpose:\", q_W_given_ZA.shape)  # Debug\n",
    "    \n",
    "    # Check dimensions and adjust if necessary\n",
    "    num_ZA = p_W_given_epsilon.shape[1]\n",
    "    num_epsilon = p_W_given_epsilon.shape[0]\n",
    "    num_W = q_W_given_ZA.shape[1]\n",
    "    \n",
    "    if q_W_given_ZA.shape[0] != num_ZA:\n",
    "        raise ValueError(\"Dimensions of q_W_given_ZA and p_W_given_epsilon do not match after transpose.\")\n",
    "    \n",
    "    # Solve by least squares\n",
    "    q_epsilon_given_Z_and_A, _, _, _ = np.linalg.lstsq(p_W_given_epsilon, q_W_given_ZA, rcond=None)\n",
    "    print(\"q_epsilon_given_Z_and_A shape after lstsq:\", q_epsilon_given_Z_and_A.shape)  # Debug\n",
    "    return q_epsilon_given_Z_and_A.T\n",
    "\n",
    "\n",
    "def volume_regularized_nmf(B, n_components, w_vol=0.1, delta=1e-8, n_iter=1000, err_cut=1e-8):\n",
    "    \"\"\"\n",
    "    Volume-regularized NMF implementation in Python.\n",
    "    \n",
    "    Args:\n",
    "        B (np.ndarray): Input matrix to factorize.\n",
    "        n_components (int): Number of components for factorization.\n",
    "        w_vol (float): Weight for volume regularization.\n",
    "        delta (float): Regularization term for logdet.\n",
    "        n_iter (int): Number of iterations.\n",
    "        err_cut (float): Convergence criterion.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Factorized matrices C, R.\n",
    "    \"\"\"\n",
    "    #np.random.seed(0)  # For reproducibility\n",
    "\n",
    "    # Initialize C and R using standard NMF\n",
    "    nmf = NMF(n_components=n_components, init='random', random_state=0, max_iter=500)\n",
    "    C = nmf.fit_transform(B)\n",
    "    R = nmf.components_\n",
    "\n",
    "    for iteration in range(n_iter):\n",
    "        # Update R using volume regularization\n",
    "        err_prev = np.linalg.norm(B - np.dot(C, R), 'fro')**2\n",
    "\n",
    "        # Update R with regularization\n",
    "        R = np.maximum(np.dot(pinv(C), B), 0)  # Standard NMF update\n",
    "        R = R - w_vol * (np.dot(R, R.T) + delta * np.eye(R.shape[0]))  # Volume regularization update\n",
    "        R = np.maximum(R, 0)  # Ensure non-negativity\n",
    "\n",
    "        # Update C to satisfy the simplex constraint\n",
    "        C = np.maximum(np.dot(B, pinv(R)), 0)\n",
    "        C = C / C.sum(axis=0)\n",
    "\n",
    "        err_post = np.linalg.norm(B - np.dot(C, R), 'fro')**2\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.abs(err_prev - err_post) / err_prev < err_cut:\n",
    "            break\n",
    "\n",
    "    return C, R\n",
    "\n",
    "def volnmf_logdet(C, X, R, R_constraint=\"pos\", majorate=False, extrapolate=True, qmax=100,\n",
    "                  w_vol=1e-1, delta=1, err_cut=1e-3, n_iter=1000):\n",
    "    # Initial assignments\n",
    "    W = np.transpose(R)\n",
    "    W_update = W.copy()\n",
    "    H = np.transpose(C)\n",
    "    FM = np.linalg.inv(np.transpose(W) @ W + delta * np.eye(W.shape[0]))\n",
    "\n",
    "    # Iteration variables\n",
    "    iter = 1\n",
    "    err = 1e5\n",
    "    q = [1, (1 + np.sqrt(5)) / 5]\n",
    "\n",
    "    # Main iteration loop\n",
    "    while err > err_cut and iter < n_iter:\n",
    "        W_prev = W.copy()\n",
    "        \n",
    "        if majorate:\n",
    "            Y = W_prev\n",
    "            FM = np.linalg.inv(np.transpose(Y) @ Y + delta * np.eye(Y.shape[0]))\n",
    "        \n",
    "        if R_constraint == \"pos\":\n",
    "            Lip = np.sqrt(np.sum((H @ H.T + w_vol * FM) ** 2))\n",
    "            gradF = W_update @ (H @ H.T + w_vol * FM) - X.T @ H\n",
    "            W = W_update - gradF / Lip\n",
    "            W[W < 0] = 0\n",
    "        else:\n",
    "            W = X.T @ H.T @ np.linalg.inv(H @ H.T + w_vol * FM)\n",
    "\n",
    "        if extrapolate:\n",
    "            extr = (q[iter - 1] - 1) / q[iter]\n",
    "            W_update = W + extr * (W - W_prev)\n",
    "        else:\n",
    "            W_update = W\n",
    "\n",
    "        # Error calculation and iteration increment\n",
    "        err = np.sum((W - W_prev) ** 2) / np.sum(W ** 2)\n",
    "        iter += 1\n",
    "        q.append(min(qmax, (1 + np.sqrt(1 + 4 * q[iter - 1] ** 2)) / 2))\n",
    "\n",
    "    return W.T\n",
    "\n",
    "def volnmf_estimate(B, C, R, Q, domain=\"covariance\", volf='logdet', R_majorate=False,\n",
    "                    wvol=None, delta=1e-8, n_iter=1000, err_cut=1e-8,\n",
    "                    vol_iter=100, c_iter=100,\n",
    "                    extrapolate=True, accelerate=True, acc_C=4/5, acc_R=3/4,\n",
    "                    C_constraint=\"col\", C_bound=1, R_constraint=\"pos\",\n",
    "                    verbose=True, record=100, Ctrue=None, mutation_run=False):\n",
    "    \n",
    "    # Initialization\n",
    "    iter = 1\n",
    "    err = 1e5\n",
    "    rvol = []\n",
    "    aff_mean = []\n",
    "    info_record = []\n",
    "    eigens = 1\n",
    "    R_update = R.copy()\n",
    "    C_update = C.copy()\n",
    "    tot_update_prev = 0\n",
    "    tot_update = 0\n",
    "\n",
    "    while iter < n_iter and err > err_cut:\n",
    "        # Domain check\n",
    "        if domain == \"covariance\":\n",
    "            X = np.dot(B, Q)\n",
    "        else:\n",
    "            X = B\n",
    "\n",
    "        # Update R\n",
    "        err_prev = np.sum((X - np.dot(C_update, R))**2)\n",
    "        if volf == \"logdet\":\n",
    "            vol_prev = np.log(np.linalg.det(np.dot(R, R.T) + delta * np.eye(R.shape[0])))\n",
    "        elif volf == \"det\":\n",
    "            vol_prev = np.linalg.det(np.dot(R, R.T))\n",
    "        R_prev = R.copy()\n",
    "\n",
    "        # Update R based on the volume function\n",
    "        if volf == \"logdet\":\n",
    "            R = volnmf_logdet(C_update, X, R_update, R_constraint=R_constraint, extrapolate=extrapolate, majorate=R_majorate,\n",
    "                              w_vol=wvol, delta=delta, err_cut=1e-100, n_iter=vol_iter)\n",
    "        elif volf == \"det\":\n",
    "            R = volnmf_det(C_update, X, R_update, posit=False, w_vol=wvol, eigen_cut=1e-20, err_cut=1e-100, n_iter=vol_iter)\n",
    "\n",
    "        ### Post-update calculations\n",
    "        err_post = np.sum((X - np.dot(C_update, R))**2)\n",
    "        if volf == \"logdet\":\n",
    "            vol_post = np.log(np.linalg.det(np.dot(R, R.T) + delta * np.eye(R.shape[0])))\n",
    "        elif volf == \"det\":\n",
    "            vol_post = np.linalg.det(np.dot(R, R.T))\n",
    "        rvol.append(vol_post)\n",
    "\n",
    "        ### Update C\n",
    "        C_prev = C.copy()\n",
    "        if C_constraint == \"col\":\n",
    "            C = volnmf_simplex_col(X, R, C_prev=C_update, bound=C_bound, extrapolate=extrapolate, err_cut=1e-100, n_iter=c_iter)\n",
    "        else:\n",
    "            C = volnmf_simplex_row(X, R, C_prev=C_update, meq=1)\n",
    "        err_post_C = np.sum((X - np.dot(C, R_update))**2)\n",
    "\n",
    "        # Accelerate C if possible\n",
    "        if accelerate:\n",
    "            C_update = C + acc_C * (C - C_prev)\n",
    "            R_update = R + acc_R * (R - R_prev)\n",
    "\n",
    "            # Ensure non-negativity\n",
    "            C_update = np.maximum(C_update, 0)\n",
    "            R_update = np.maximum(R_update, 0)\n",
    "\n",
    "            err_update = np.sum((X - np.dot(C, R_update))**2)\n",
    "            vol_update = np.log(np.linalg.det(np.dot(R_update, R_update.T) + delta * np.eye(R_update.shape[0])))\n",
    "            tot_update = err_update + wvol * vol_update\n",
    "\n",
    "            if tot_update > tot_update_prev:\n",
    "                C_update = C.copy()\n",
    "                R_update = R.copy()\n",
    "        else:\n",
    "            C_update = C.copy()\n",
    "            R_update = R.copy()\n",
    "\n",
    "        tot_update_prev = tot_update\n",
    "\n",
    "        ### optimize Q\n",
    "        if domain == \"covariance\":\n",
    "            Q_init = volnmf_procrustes(np.dot(C_update, R_update), B)\n",
    "\n",
    "        err = np.sum((C_update - C_prev)**2) / np.sum(C_update**2)\n",
    "        eigens = np.linalg.eigvals(np.dot(R_update, R_update.T))\n",
    "        aff = 1\n",
    "\n",
    "        if Ctrue is not None:\n",
    "            if C_bound is None:\n",
    "                aff = np.apply_along_axis(lambda x: np.max(np.abs(np.corrcoef(x, Ctrue))), axis=1, arr=C_update)\n",
    "            else:\n",
    "                aff = np.apply_along_axis(lambda x: np.max(np.abs(np.corrcoef(x * C_bound, Ctrue))), axis=1,\n",
    "                                          arr=C_update)\n",
    "\n",
    "        aff_mean.append(np.mean(aff))\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "    return {'C': C_update, 'R': R_update, 'Q': Q_init, 'iter': iter, 'err': err, 'info_record': info_record}\n",
    "\n",
    "def volnmf_main(vol, B, volnmf=None, n_comp=3, n_reduce=None,\n",
    "                do_nmf=True, iter_nmf=100, seed=None,\n",
    "                domain=\"covariance\", volf='logdet',\n",
    "                wvol=None, delta=1e-8, n_iter=500, err_cut=1e-16,\n",
    "                vol_iter=20, c_iter=20,\n",
    "                extrapolate=True, accelerate=False, acc_C=4/5, acc_R=3/4,\n",
    "                C_constraint=\"col\", C_bound=1, R_constraint=\"pos\", R_majorate=False,\n",
    "                C_init=None, R_init=None, Q_init=None, anchor=None, Ctrue=None,\n",
    "                verbose=True, record=100, verbose_nmf=False, record_nmf=None, mutation_run=False):\n",
    "\n",
    "    # Initialize Q_init if None\n",
    "    if Q_init is None:\n",
    "        Q_init = np.eye(n_reduce, n_comp)  # Identity matrix with n_reduce rows and n_comp columns\n",
    "\n",
    "    # Set random seed if provided\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Use NMF to initialize C_init and R_init if they are not provided\n",
    "    if C_init is None or R_init is None:\n",
    "        nmf_model = NMF(n_components=n_comp, init='random', random_state=seed)\n",
    "        C_init = nmf_model.fit_transform(B)\n",
    "        R_init = nmf_model.components_\n",
    "\n",
    "    C_rand, R_rand, Q_rand = C_init.copy(), R_init.copy(), Q_init.copy()\n",
    "\n",
    "    if wvol is None:\n",
    "        wvol = 0\n",
    "\n",
    "    # Print message indicating the start of volume-regularized NMF\n",
    "    print('Run volume-regularized NMF...')\n",
    "\n",
    "    # Run volume-regularized NMF\n",
    "    vol_solution = volnmf_estimate(B, C_init, R_init, Q_init,\n",
    "                                   domain=domain, volf=volf, R_majorate=R_majorate,\n",
    "                                   wvol=wvol, delta=delta, n_iter=n_iter, err_cut=err_cut,\n",
    "                                   vol_iter=vol_iter, c_iter=c_iter,\n",
    "                                   extrapolate=extrapolate, accelerate=accelerate, acc_C=acc_C, acc_R=acc_R,\n",
    "                                   C_constraint=C_constraint, C_bound=C_bound, R_constraint=R_constraint,\n",
    "                                   verbose=verbose, record=record, Ctrue=Ctrue, mutation_run=mutation_run)\n",
    "    \n",
    "    # Print done message\n",
    "    print('Done')\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        'C': vol_solution['C'], 'R': vol_solution['R'], 'Q': vol_solution['Q'],\n",
    "        'C_init': C_init, 'R_init': R_init, 'Q_init': Q_init,\n",
    "        'C_rand': C_rand, 'R_rand': R_rand, 'Q_rand': Q_rand,\n",
    "        'rec': vol_solution['info_record']\n",
    "    }\n",
    "\n",
    "def plot_histograms(source_data):\n",
    "    Z_source, epsilon_source, W_source, A_source, Y_source = source_data\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    Z_source_np = Z_source.numpy()\n",
    "    epsilon_source_np = epsilon_source.numpy()\n",
    "    W_source_np = W_source.numpy()\n",
    "    A_source_np = A_source.numpy()\n",
    "    Y_source_np = Y_source.numpy()\n",
    "\n",
    "    # Flatten the arrays for histogram plotting\n",
    "    Z_source_flat = Z_source_np.flatten()\n",
    "    epsilon_source_flat = epsilon_source_np.flatten()\n",
    "    W_source_flat = W_source_np.flatten()\n",
    "    A_source_flat = A_source_np.flatten()\n",
    "    Y_source_flat = Y_source_np.flatten()\n",
    "\n",
    "    # Plot histograms\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(Z_source_flat, bins=20, color='blue', alpha=0.7)\n",
    "    plt.title('Histogram of Z_source')\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(epsilon_source_flat, bins=20, color='green', alpha=0.7)\n",
    "    plt.title('Histogram of epsilon_source')\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.hist(W_source_flat, bins=20, color='red', alpha=0.7)\n",
    "    plt.title('Histogram of W_source')\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.hist(A_source_flat, bins=20, color='purple', alpha=0.7)\n",
    "    plt.title('Histogram of A_source')\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.hist(Y_source_flat, bins=20, color='orange', alpha=0.7)\n",
    "    plt.title('Histogram of Y_source')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_tuple(theta_xz,theta_yx,theta_yw,p=0.2):\n",
    "    vec1 = torch.tensor([0.1,0.1,0.4,0.4])\n",
    "    vec2 = torch.tensor([0.4,0.4,0.1,0.1])\n",
    "\n",
    "    U = torch.bernoulli(torch.tensor([p])).item()\n",
    "\n",
    "    cat0 = torch.distributions.categorical.Categorical(vec1) \n",
    "    cat1 = torch.distributions.categorical.Categorical(vec2) \n",
    "\n",
    "    z = torch.nn.functional.one_hot((cat0.sample()*U + cat1.sample()*(1-U)).long(), num_classes = 4).unsqueeze(0).float()\n",
    "    w = torch.nn.functional.one_hot((cat0.sample()*U + cat1.sample()*(1-U)).long(), num_classes = 4).unsqueeze(0).float()\n",
    "\n",
    "    probx = (vec1 * U + vec2 * (1-U)).unsqueeze(0)\n",
    "    proby = (vec1 * U + vec2 * (1-U)).unsqueeze(0)\n",
    "\n",
    "    probx = probx * (theta_xz @ z.T).T\n",
    "    #print(probx)\n",
    "    x = torch.nn.functional.one_hot(torch.distributions.categorical.Categorical(probx).sample().long(), num_classes = 4).float()\n",
    "    #print(x)\n",
    "    # x = torch.nn.functional.one_hot(torch.argmax(probx, dim=1).long(), num_classes = 4).float()\n",
    "\n",
    "    proby = proby * (theta_yx @ x.T + theta_yw @ w.T).T\n",
    "    y_prob = proby.sum(dim=1)\n",
    "    y = (y_prob > 0.5).float().unsqueeze(0)\n",
    "\n",
    "    return U,z,w,x,y\n",
    "\n",
    "def get_dataset(theta_a_z,theta_y_a,theta_y_w, p, total):\n",
    "    U,Z,W,X,Y = [],[],[],[],[]\n",
    "    for _ in range(total):\n",
    "        u,z,w,x,y = get_tuple(theta_a_z,theta_y_a,theta_y_w,p)\n",
    "        U.append(torch.tensor([[u]]))\n",
    "        Z.append(z)\n",
    "        W.append(w)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    # print(u.shape,z.shape,w.shape,x.shape,y.shape)\n",
    "    U = torch.cat(U,0)\n",
    "    Z = torch.cat(Z,0)\n",
    "    W = torch.cat(W,0)\n",
    "    X = torch.cat(X,0)\n",
    "    Y = torch.cat(Y,0)\n",
    "\n",
    "    return U,Z,W,X,Y\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    get_tuple(torch.rand(4,4),torch.rand(4,4),torch.rand(4,4),0.2)\n",
    "    #print(get_dataset(torch.rand(4,4),torch.rand(4,4),torch.rand(4,4),0.2,10))\n",
    "\n",
    "\n",
    "def get_tuple_new(theta_a_z,theta_y_a,theta_y_w,theta_y_epsilon, theta_a_epsilon, p=0.2):\n",
    "    vec1 = torch.tensor([0.1,0.1,0.4,0.4])\n",
    "    vec2 = torch.tensor([0.4,0.4,0.1,0.1])\n",
    "\n",
    "    epsilon = torch.bernoulli(torch.tensor([p])).float()\n",
    "\n",
    "    cat0 = torch.distributions.categorical.Categorical(vec1) \n",
    "    cat1 = torch.distributions.categorical.Categorical(vec2) \n",
    "\n",
    "    z = torch.nn.functional.one_hot((cat0.sample()*epsilon + cat1.sample()*(1-epsilon)).long(), num_classes = 4).unsqueeze(0).float() #discrete so categorical depending on epsilon (paper has Normal as cont)\n",
    "    w = torch.nn.functional.one_hot((cat0.sample()*epsilon + cat1.sample()*(1-epsilon)).long(), num_classes = 4).unsqueeze(0).float() #discrete so categorical depending on epsilon (paper has Normal as cont)\n",
    "\n",
    "    proba = (vec1 * epsilon + vec2 * (1-epsilon)).unsqueeze(0)  \n",
    "    proby = (vec1 * epsilon + vec2 * (1-epsilon)).unsqueeze(0)\n",
    "    \n",
    "    print(\"proba shape\",proba.shape)\n",
    "    print(\"theta_a_z shape\",theta_a_z.shape)\n",
    "    print(\"z.T shape\",z.T.shape)\n",
    "    print(\"theta_a_epsilon shape\",theta_a_epsilon.shape)\n",
    "    print(\"epsilon.T shape\",epsilon.T.shape)\n",
    "\n",
    "\n",
    "    proba = proba * (theta_a_z @ z.T + theta_a_epsilon * epsilon.T).T # a is a function of z and epsilon from the graph\n",
    "    a = torch.bernoulli(torch.sigmoid(proba).sample().long(), num_classes = 4).float()\n",
    "\n",
    "    proby = proby * (theta_y_a @ a.T + theta_y_w @ w.T + theta_y_epsilon * epsilon.T).T # y is a function of a, w and epsilon from the graph\n",
    "    #y = torch.bernoulli(torch.nn.Sigmoid(proby).sample().long(), num_classes = 4).float()\n",
    "    y = torch.bernoulli(torch.sigmoid(proby).squeeze()).float()\n",
    "\n",
    "    return epsilon,z,w,a,y\n",
    "\n",
    "def get_dataset_new(theta_a_z,theta_y_a,theta_y_w,theta_y_epsilon, theta_a_epsilon, p, total):\n",
    "    U,Z,W,X,Y = [],[],[],[],[]\n",
    "    for _ in range(total):\n",
    "        u,z,w,x,y = get_tuple_new(theta_a_z,theta_y_a,theta_y_w,theta_y_epsilon, theta_a_epsilon, p)\n",
    "        U.append(torch.tensor([[u]]))\n",
    "        Z.append(z)\n",
    "        W.append(w)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    # print(u.shape,z.shape,w.shape,x.shape,y.shape)\n",
    "    U = torch.cat(U,0)\n",
    "    Z = torch.cat(Z,0)\n",
    "    W = torch.cat(W,0)\n",
    "    X = torch.cat(X,0)\n",
    "    Y = torch.cat(Y,0)\n",
    "\n",
    "    return U,Z,W,X,Y\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    get_tuple(torch.rand(4,4),torch.rand(4,4),torch.rand(4,4),0.2)\n",
    "    #print(get_dataset(torch.rand(4,4),torch.rand(4,4),torch.rand(4,4),0.2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc15d203d50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_debug = True\n",
    "step1_debug = True\n",
    "step2_debug = True\n",
    "step3_debug = True\n",
    "step4_debug = False\n",
    "step5_debug = False\n",
    "step6_debug = False\n",
    "testing = False\n",
    "parameter_tuning = False\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_source = 0.8\n",
    "p_target = 0.2\n",
    "total = 10000\n",
    "factorisation_atol = 1e-1\n",
    "\n",
    "# Step 2 parameters\n",
    "specific_a_index = 0  # First value of A\n",
    "\n",
    "# \"sklearn\" for sklearn's NMF\n",
    "# \"volmin_1\" for volmin NMF, adapted from https://github.com/kharchenkolab/vrnmf\n",
    "# \"volmin_2\" for volmin NMF, adapted from https://github.com/bm424/mvcnmf/blob/master/mvcnmf.py\n",
    "nmf_method = \"volmin_2\" \n",
    "# parameters for volmin NMF\n",
    "w_vol = 5\n",
    "delta = 1e-8\n",
    "n_iter = 100000\n",
    "err_cut = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_w_epsilon = torch.tensor([\n",
    "    [-1],\n",
    "    [1]\n",
    "])\n",
    "\n",
    "theta_y_a = torch.tensor([\n",
    "    [-1, 1, 1, 1],\n",
    "    [1, -1, 1, 1],\n",
    "    [1, 1, -1, 1],\n",
    "    [1, 1, 1, -1]\n",
    "])\n",
    "\n",
    "theta_y_w = torch.tensor([\n",
    "    [-1, 1, 1, 1],\n",
    "    [1, -1, 1, 1],\n",
    "    [1, 1, -1, 1],\n",
    "    [1, 1, 1, -1]\n",
    "])\n",
    "\n",
    "theta_y_epsilon = torch.tensor([\n",
    "    [-1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "theta_a_epsilon = torch.tensor([\n",
    "    [-1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "theta_a_z = torch.tensor([\n",
    "    [-1, 1, 1, 1],\n",
    "    [1, -1, 1, 1],\n",
    "    [1, 1, -1, 1],\n",
    "    [1, 1, 1, -1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuple_new(theta_a_z,theta_y_a,theta_y_w,theta_y_epsilon, theta_a_epsilon, p=0.2):\n",
    "    vec1 = torch.tensor([0.1,0.1,0.4,0.4]) # (4,)\n",
    "    vec2 = torch.tensor([0.4,0.4,0.1,0.1]) # (4,)\n",
    "\n",
    "    epsilon = torch.bernoulli(torch.tensor([p])).long() # (1,)\n",
    "    epsilon_one_hot = torch.nn.functional.one_hot(torch.tensor(epsilon), num_classes=2).squeeze() # (1,2)\n",
    "\n",
    "    cat0 = torch.distributions.categorical.Categorical(vec1) \n",
    "    cat1 = torch.distributions.categorical.Categorical(vec2) \n",
    "\n",
    "    z = torch.nn.functional.one_hot((cat0.sample()*epsilon + cat1.sample()*(1-epsilon)).long(), num_classes = 4).squeeze() #(1,4) #discrete so categorical depending on epsilon (paper has Normal as cont)\n",
    "    w = torch.nn.functional.one_hot((cat0.sample()*epsilon + cat1.sample()*(1-epsilon)).long(), num_classes = 4).squeeze() #(1,4) #discrete so categorical depending on epsilon (paper has Normal as cont)\n",
    "\n",
    "    ###### FIX FROM HERE ######\n",
    "    # z is (1,4), theta_a_z is (4,4), theta_a_epsilon is (4,1)\n",
    "    a_logits = (z.float() @ theta_a_z.float()  + theta_a_epsilon * epsilon.T) # a is a function of z and epsilon from the graph \n",
    "    a = torch.bernoulli(torch.sigmoid(a_logits)).float().squeeze() # (1,4)\n",
    "\n",
    "    # a is (1,4), theta_y_a is (4,4), w is (1,4), theta_y_w is (4,4), theta_y_epsilon is (1,4), epsilon is (1,)\n",
    "    y_logits = (a.float() @ theta_y_a.float()   + w.float() @ theta_y_w.float() + theta_y_epsilon.float() * epsilon) # y is a function of a, w and epsilon from the graph\n",
    "    y = torch.bernoulli(torch.sigmoid(y_logits).squeeze()).float()\n",
    "    print(y)\n",
    "\n",
    "    return epsilon,z,w,a,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/g242212x4tsbkxq6k9xkfvn80000gn/T/ipykernel_14924/3287227568.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  epsilon_one_hot = torch.nn.functional.one_hot(torch.tensor(epsilon), num_classes=2).squeeze() # (1,2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 0., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 0.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([0., 0., 0., 1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 0., 1.])\n",
      "tensor([1., 1., 1., 0.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m source_data, target_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_a_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_y_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_y_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_y_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_a_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m Z_source, epsilon_source, W_source, A_source, Y_source \u001b[38;5;241m=\u001b[39m source_data\n\u001b[1;32m      3\u001b[0m Z_target, epsilon_target, W_target, A_target, Y_target \u001b[38;5;241m=\u001b[39m target_data\n",
      "Cell \u001b[0;32mIn[49], line 48\u001b[0m, in \u001b[0;36mget_data_new\u001b[0;34m(theta_a_z, theta_y_a, theta_y_w, theta_y_epsilon, theta_a_epsilon, p_source, p_target, total)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mGenerates datasets for source and target distributions.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    tuple: Two tuples containing the source and target datasets respectively.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Source distribution data\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m U_source, Z_source, W_source, X_source, Y_source \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_a_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_y_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_y_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_y_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_a_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Target distribution data\u001b[39;00m\n\u001b[1;32m     51\u001b[0m U_target, Z_target, W_target, X_target, Y_target \u001b[38;5;241m=\u001b[39m get_dataset_new(theta_a_z, theta_y_a, theta_y_w, theta_y_epsilon, theta_a_epsilon, p_target, total)\n",
      "Cell \u001b[0;32mIn[50], line 87\u001b[0m, in \u001b[0;36mget_dataset_new\u001b[0;34m(theta_a_z, theta_y_a, theta_y_w, theta_y_epsilon, theta_a_epsilon, p, total)\u001b[0m\n\u001b[1;32m     85\u001b[0m U,Z,W,X,Y \u001b[38;5;241m=\u001b[39m [],[],[],[],[]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total):\n\u001b[0;32m---> 87\u001b[0m     u,z,w,x,y \u001b[38;5;241m=\u001b[39m \u001b[43mget_tuple_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_a_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_y_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_y_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_y_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_a_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     U\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor([[u]]))\n\u001b[1;32m     89\u001b[0m     Z\u001b[38;5;241m.\u001b[39mappend(z)\n",
      "Cell \u001b[0;32mIn[178], line 22\u001b[0m, in \u001b[0;36mget_tuple_new\u001b[0;34m(theta_a_z, theta_y_a, theta_y_w, theta_y_epsilon, theta_a_epsilon, p)\u001b[0m\n\u001b[1;32m     20\u001b[0m y_logits \u001b[38;5;241m=\u001b[39m (a\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m@\u001b[39m theta_y_a\u001b[38;5;241m.\u001b[39mfloat()   \u001b[38;5;241m+\u001b[39m w\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m@\u001b[39m theta_y_w\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m+\u001b[39m theta_y_epsilon\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m epsilon) \u001b[38;5;66;03m# y is a function of a, w and epsilon from the graph\u001b[39;00m\n\u001b[1;32m     21\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(torch\u001b[38;5;241m.\u001b[39msigmoid(y_logits)\u001b[38;5;241m.\u001b[39msqueeze())\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epsilon,z,w,a,y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MV00/lib/python3.9/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MV00/lib/python3.9/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MV00/lib/python3.9/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MV00/lib/python3.9/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MV00/lib/python3.9/site-packages/torch/_tensor_str.py:151\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    148\u001b[0m nonzero_finite_max \u001b[38;5;241m=\u001b[39m tensor_totype(nonzero_finite_abs\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "source_data, target_data = get_data_new(theta_a_z, theta_y_a, theta_y_w, theta_y_epsilon, theta_a_epsilon, p_source, p_target, total)\n",
    "Z_source, epsilon_source, W_source, A_source, Y_source = source_data\n",
    "Z_target, epsilon_target, W_target, A_target, Y_target = target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_source.shape (40000,)\n",
      "W_source.shape (40000,)\n",
      "A_source.shape (40000,)\n",
      "Z_source.shape (40000,)\n",
      "Y_source [1. 1. 0. ... 0. 0. 1.]\n",
      "W_source [0 0 1 ... 0 0 1]\n",
      "A_source [0. 1. 1. ... 0. 1. 0.]\n",
      "Z_source [0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_source.shape\", Y_source.shape)\n",
    "print(\"W_source.shape\", W_source.shape)\n",
    "print(\"A_source.shape\", A_source.shape)\n",
    "print(\"Z_source.shape\", Z_source.shape)\n",
    "\n",
    "print(\"Y_source\", Y_source)\n",
    "print(\"W_source\", W_source)\n",
    "print(\"A_source\", A_source)\n",
    "print(\"Z_source\", Z_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(source_data):\n",
    "    Z_source, epsilon_source, W_source, A_source, Y_source = source_data\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    Z_source_np = Z_source\n",
    "    epsilon_source_np = epsilon_source\n",
    "    W_source_np = W_source\n",
    "    A_source_np = A_source\n",
    "    Y_source_np = Y_source\n",
    "\n",
    "    # Flatten the arrays for histogram plotting\n",
    "    Z_source_flat = Z_source_np.flatten()\n",
    "    epsilon_source_flat = epsilon_source_np.flatten()\n",
    "    W_source_flat = W_source_np.flatten()\n",
    "    A_source_flat = A_source_np.flatten()\n",
    "    Y_source_flat = Y_source_np.flatten()\n",
    "\n",
    "    # Plot histograms\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(Z_source_flat, bins=20, color='blue', alpha=0.7)\n",
    "    plt.title('Histogram of Z_source')\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(epsilon_source_flat, bins=20, color='green', alpha=0.7)\n",
    "    plt.title('Histogram of epsilon_source')\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.hist(W_source_flat, bins=20, color='red', alpha=0.7)\n",
    "    plt.title('Histogram of W_source')\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.hist(A_source_flat, bins=20, color='purple', alpha=0.7)\n",
    "    plt.title('Histogram of A_source')\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.hist(Y_source_flat, bins=20, color='orange', alpha=0.7)\n",
    "    plt.title('Histogram of Y_source')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_source.shape)\n",
    "print(W_source.shape)\n",
    "print(A_source.shape)\n",
    "print(Z_source.shape)\n",
    "\n",
    "print(A_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(Z_source, W_source, A_source, Y_source):\n",
    "    combinations = list(itertools.product(range(4), range(4)))\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    # Convert one-hot encoded tensors to indices directly\n",
    "    Z_indices = np.argmax(Z_source, axis=1).flatten()\n",
    "    A_indices = np.argmax(A_source, axis=1).flatten()\n",
    "    Y_indices = Y_source.flatten()  # Y_source is already binary, just flatten it\n",
    "    W_indices = np.argmax(W_source, axis=1).flatten()\n",
    "\n",
    "    print(Y_indices.shape)\n",
    "    \n",
    "    # Print the first few indices for debugging\n",
    "    print(\"Z_indices (first 10):\", Z_indices[:10])\n",
    "    print(\"A_indices (first 10):\", A_indices[:10])\n",
    "    print(\"Y_indices (first 10):\", Y_indices[:10])\n",
    "    print(\"W_indices (first 10):\", W_indices[:10])\n",
    "    \n",
    "    for i, (z_val, a_val) in enumerate(combinations):\n",
    "        indices = (Z_indices == z_val) & (A_indices == a_val)\n",
    "        \n",
    "        Y_subsample = Y_indices[indices]\n",
    "        W_subsample = W_indices[indices]\n",
    "        \n",
    "        plt.subplot(8, 4, 2 * i + 1)\n",
    "        plt.hist(Y_subsample, bins=np.arange(3) - 0.5, color='orange', alpha=0.7)\n",
    "        plt.title(f'Y | Z={z_val}, A={a_val}')\n",
    "        plt.xticks([0, 1])\n",
    "        \n",
    "        plt.subplot(8, 4, 2 * i + 2)\n",
    "        plt.hist(W_subsample, bins=np.arange(5) - 0.5, color='blue', alpha=0.7)\n",
    "        plt.title(f'W | Z={z_val}, A={a_val}')\n",
    "        plt.xticks([0, 1, 2, 3])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_distributions(Z_source, W_source, A_source, Y_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Z_source shape:\", Z_source.shape)\n",
    "print(\"A_source shape:\", A_source.shape)\n",
    "print(\"W_source shape:\", W_source.shape)\n",
    "print(\"epsilon_source shape:\", epsilon_source.shape)\n",
    "print(\"Y_source shape:\", Y_source.shape)\n",
    "print(\"Z_source:\", Z_source)\n",
    "print(\"A_source:\", A_source)\n",
    "print(\"W_source:\", W_source)\n",
    "print(\"epsilon_source:\", epsilon_source)\n",
    "print(\"Y_source:\", Y_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_Y = 2 #AMEND THIS???????????????????????????????\n",
    "num_classes_W = W_source.shape[1]\n",
    "num_features_Z = Z_source.shape[1]\n",
    "num_features_A = A_source.shape[1]\n",
    "\n",
    "sum_epsilon = np.sum(epsilon_source)\n",
    "print(\"Sum of epsilon_source:\", sum_epsilon.item())\n",
    "\n",
    "sum_Y = np.sum(Y_source)\n",
    "print(\"Sum of Y_source:\", sum_Y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Estimate p(Y|Z,a) and p(W|Z,a)\n",
    "# =============================================================================\n",
    "\n",
    "# Train model to estimate p(Y|Z,a)\n",
    "# By stacking with A, we condition on A by including all values of A in the input\n",
    "ZA_source = np.hstack((Z_source, A_source))  # We go from 4 features to 4 + 4 = 8 features\n",
    "if step1_debug:\n",
    "    print(\"ZA_source.shape\", ZA_source.shape)  # Debug print statement\n",
    "    print(\"ZA_source\", ZA_source)  # Debug print statement\n",
    "\n",
    "############### LOGISTIC REGRESSION VERSION ###############\n",
    "\n",
    "#model_Y = LogisticRegression(input_dim=ZA_source.shape[1], num_classes=2)\n",
    "#model_Y.train(torch.tensor(ZA_source, dtype=torch.float32), torch.tensor(Y_source, dtype=torch.float32))\n",
    "\n",
    "# model_Y = LogisticRegressionGD(input_dim=ZA_source.shape[1], num_classes=Y_source.shape[1])\n",
    "# model_Y.train_model(torch.tensor(ZA_source, dtype=torch.float32), torch.tensor(Y_source, dtype=torch.float32), learning_rate=0.01, epochs=100, verbose=True)\n",
    "# p_Y_given_ZA = get_probabilities(model_Y, Z_source, A_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "def train_nn(model, X, Y, learning_rate=0.001, epochs=50, batch_size=32):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_X, batch_Y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "def eval_nn(model, X, Y):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == Y).sum().item()\n",
    "        accuracy = correct / Y.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the neural network\n",
    "input_dim = ZA_source.shape[1]\n",
    "model_Y = SimpleNN(input_dim, num_classes_Y)\n",
    "train_nn(model_Y, torch.tensor(ZA_source, dtype=torch.float32), torch.tensor(Y_source.flatten(), dtype=torch.long), learning_rate=0.001, epochs=500, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_nn = eval_nn(model_Y, torch.tensor(ZA_source, dtype=torch.float32), torch.tensor(Y_source.flatten(), dtype=torch.long))\n",
    "print(f'Accuracy of neural network model on training set: {accuracy_nn:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ZA_source.shape)\n",
    "print(Y_source.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_source.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### SKLEARN VERSION ###############\n",
    "\n",
    "print(\"Y_source flatten\", Y_source.flatten())\n",
    "\n",
    "##### LOGISTIC REGRESSION #####\n",
    "#model_Y = SklearnLogisticRegression(max_iter=1000)\n",
    "#model_Y.fit(ZA_source, Y_source.flatten())\n",
    "\n",
    "##### GRAIDENT BOOSTING #####\n",
    "#model_Y = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01, max_depth=5, random_state=0)\n",
    "#model_Y.fit(ZA_source, Y_source.flatten())\n",
    "\n",
    "##### RANDOM FOREST #####\n",
    "model_Y = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)\n",
    "model_Y.fit(ZA_source, Y_source.flatten())\n",
    "\n",
    "##### NEURAL NETWORK #####\n",
    "#model_Y = MLPClassifier(hidden_layer_sizes=(8, 4), max_iter=1000)\n",
    "#model_Y.fit(ZA_source, Y_source.flatten())\n",
    "\n",
    "\n",
    "print(ZA_source)\n",
    "print(Y_source.flatten())\n",
    "Y_train_pred = model_Y.predict(ZA_source)\n",
    "print(\"Y_train_pred\",Y_train_pred)\n",
    "\n",
    "Y_train_true = Y_source\n",
    "accuracy_Y_train = np.mean(Y_train_pred == Y_source)\n",
    "print(f\"Accuracy of model_Y on training set: {accuracy_Y_train:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_source.flatten(), model_Y.predict(ZA_source))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_source.flatten(), model_Y.predict(ZA_source)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities_one_hot_nn(model, Z, A):\n",
    "    \"\"\"\n",
    "    Computes the softmax probabilities from the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model (model): Trained model.\n",
    "        Z (numpy.ndarray): Feature matrix Z.\n",
    "        A (numpy.ndarray): Feature matrix A.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Probability matrix reshaped to (|Z|, |A|, |Y|) or (|Z|, |A|, |W|).\n",
    "    \"\"\"\n",
    "    num_Z = Z.shape[1]\n",
    "    num_A = A.shape[1]\n",
    "    num_classes = model.fc5.out_features #number of layers here\n",
    "\n",
    "    # Generate all possible one-hot vectors for Z\n",
    "    possible_Z = np.eye(num_Z)\n",
    "    possible_A = np.eye(num_A)\n",
    "    \n",
    "    probabilities = []\n",
    "    \n",
    "    for z in possible_Z:\n",
    "        for a in possible_A:\n",
    "            ZA = np.hstack((z.reshape(1, -1), a.reshape(1, -1)))\n",
    "            ZA_tensor = torch.tensor(ZA, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                probs = model(ZA_tensor)\n",
    "                probs = torch.softmax(probs, dim=1).numpy()\n",
    "                probabilities.append(probs[0])\n",
    "    \n",
    "    probabilities = np.array(probabilities).reshape((num_Z, num_A, num_classes))\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_Y_given_ZA = get_probabilities_one_hot_nn(model_Y, Z_source, A_source)\n",
    "print(\"p_Y_given_ZA\", p_Y_given_ZA)\n",
    "print(\"p_Y_given_ZA shape:\", p_Y_given_ZA.shape)  # Debug print statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify the shape of p_Y_given_ZA\n",
    "assert p_Y_given_ZA.shape == (num_features_Z, num_features_A, num_classes_Y), f\"p_Y_given_ZA shape mismatch: {p_Y_given_ZA.shape}\"\n",
    "assert np.allclose(p_Y_given_ZA.sum(axis=2), 1.0), \"p_Y_given_ZA rows do not sum to 1\"\n",
    "print(\"Step 1: p_Y_given_ZA shape and sum are correct.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_source_indices = torch.argmax(torch.tensor(W_source), dim=1)\n",
    "print(W_source)\n",
    "print(W_source_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(W_source)\n",
    "# print(np.argmax(W_source, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### LOGISTIC REGRESSION VERSION ###############\n",
    "#Train model to estimate p(W|Z,a)\n",
    "# model_W = LogisticRegressionGD(input_dim=ZA_source.shape[1], num_classes=W_source.shape[1])\n",
    "# model_W.train_model(torch.tensor(ZA_source, dtype=torch.float32), torch.tensor(Y_source, dtype=torch.float32), learning_rate=0.01, epochs=100, verbose=True)\n",
    "\n",
    "# p_W_given_ZA = get_probabilities_one_hot(model_W, Z_source, A_source)\n",
    "\n",
    "\n",
    "\n",
    "############### SKLEARN VERSION ###############\n",
    "\n",
    "model_W = SklearnLogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "model_W.fit(ZA_source, np.argmax(W_source, axis=1))\n",
    "W_train_pred = model_W.predict(ZA_source)\n",
    "W_train_true = np.argmax(W_source, axis=1)\n",
    "accuracy_W_train = np.mean(W_train_pred == W_train_true)\n",
    "print(f\"Accuracy of model_W on training set: {accuracy_W_train:.4f}\")\n",
    "#p_W_given_ZA = model_W.predict_proba(ZA_source).reshape(num_features_Z, num_features_A, num_classes_W)\n",
    "\n",
    "\n",
    "\n",
    "############### NEURAL NETWORK VERSION ###############\n",
    "\n",
    "# Define and train the neural network\n",
    "# input_dim = ZA_source.shape[1]\n",
    "# model_W = SimpleNN(input_dim, num_classes_W)\n",
    "# train_nn(model_W, torch.tensor(ZA_source, dtype=torch.float32), W_source_indices, learning_rate=0.001, epochs=500, batch_size=16)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy_nn = eval_nn(model_W, torch.tensor(ZA_source, dtype=torch.float32), W_source_indices)\n",
    "# print(f'Accuracy of neural network model on training set: {accuracy_nn:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_w = confusion_matrix(W_source_indices.numpy(), model_W.predict(ZA_source))\n",
    "print(\"Classification Report for W:\")\n",
    "print(classification_report(W_source_indices.numpy(), model_W.predict(ZA_source)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for W')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_W_given_ZA = get_probabilities(model_W, Z_source, A_source)\n",
    "\n",
    "if step1_debug:\n",
    "    print(\"p_W_given_ZA shape:\", p_W_given_ZA.shape)  # Debug print statement\n",
    "    print(\"p_W_given_ZA\", p_W_given_ZA)  # Debug print statement\n",
    "\n",
    "# Verify the shape of p_W_given_ZA\n",
    "assert p_W_given_ZA.shape == (num_features_Z, num_features_A, num_classes_W), f\"p_W_given_ZA shape mismatch: {p_W_given_ZA.shape}\"\n",
    "assert np.allclose(p_W_given_ZA.sum(axis=2), 1.0), \"p_W_given_ZA rows do not sum to 1\"\n",
    "if step1_debug:\n",
    "    print(\"Step 1: p_W_given_ZA shape and sum are correct.\")\n",
    "\n",
    "print(\"STEP 1 DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MV00",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
